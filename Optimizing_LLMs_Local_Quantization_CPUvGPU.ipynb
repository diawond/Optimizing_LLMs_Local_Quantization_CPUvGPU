{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# --- Clean & Reinstall: Colab T4 (CUDA 12.6) ---\n",
        "!pip -q uninstall -y bitsandbytes triton torch torchvision torchaudio transformers\n",
        "\n",
        "# 1) PyTorch stack (cu126) — ให้ตรงกับ Colab\n",
        "!pip -q install --index-url https://download.pytorch.org/whl/cu126 \\\n",
        "  torch==2.8.0 torchvision==0.23.0 torchaudio==2.8.0\n",
        "\n",
        "# 2) Triton ให้ \"เข้าชุด\" กับ Torch 2.8 (ต้อง 3.4.0)\n",
        "!pip -q install --no-cache-dir triton==3.4.0\n",
        "\n",
        "# 3) LLM stack + bnb รุ่นใหม่ที่เข้ากับ Triton 3.x\n",
        "!pip -q install --no-cache-dir transformers==4.45.2 accelerate sentencepiece\n",
        "!pip -q install --no-cache-dir bitsandbytes==0.45.2\n",
        "\n",
        "# 4) ตรวจสอบ\n",
        "import torch, transformers, pathlib\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"Transformers:\", transformers.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "\n",
        "libs = [p.name for p in pathlib.Path(bnb.__file__).parent.glob('libbitsandbytes_cuda*.so')]\n",
        "print(\"bitsandbytes libs:\", libs)  # ต้องมีอย่างน้อย 'libbitsandbytes_cuda126.so'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzefO8WncjI5",
        "outputId": "cb12907b-d13f-40bf-b7e7-852df3fabd4c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "peft 0.17.1 requires transformers, which is not installed.\n",
            "sentence-transformers 5.1.2 requires transformers<5.0.0,>=4.41.0, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m260.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hTorch: 2.8.0+cu126\n",
            "Transformers: 4.45.2\n",
            "CUDA available: True\n",
            "bitsandbytes libs: ['libbitsandbytes_cuda120.so', 'libbitsandbytes_cuda123.so', 'libbitsandbytes_cuda126.so', 'libbitsandbytes_cuda117.so', 'libbitsandbytes_cuda121.so', 'libbitsandbytes_cuda125.so', 'libbitsandbytes_cuda118.so', 'libbitsandbytes_cuda122.so', 'libbitsandbytes_cuda124.so']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"gpt2\"  # small causal LM; good for speed tests\n",
        "PROMPT = \"In one sentence, explain why code reviews improve software quality.\"\n",
        "MAX_NEW_TOKENS = 128 #comparing between 128 and 256"
      ],
      "metadata": {
        "id": "jMnP6FKfH4g9"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time, torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "\n",
        "def count_new_tokens(tokenizer, input_text, output_text):\n",
        "    in_ids = tokenizer.encode(input_text, add_special_tokens=False)\n",
        "    out_ids = tokenizer.encode(output_text, add_special_tokens=False)\n",
        "    return max(1, len(out_ids))\n",
        "\n",
        "def run_generation(pipe, prompt, max_new_tokens=64):\n",
        "    start = time.time()\n",
        "    out = pipe(prompt, max_new_tokens=max_new_tokens, do_sample=False)\n",
        "    text = out[0]['generated_text'] if isinstance(out, list) else out[0]['generated_text']\n",
        "    elapsed = time.time() - start\n",
        "    return text, elapsed\n",
        "\n",
        "def show_gpu_mem():\n",
        "    if torch.cuda.is_available():\n",
        "        allocated = torch.cuda.memory_allocated()/1e9\n",
        "        reserved = torch.cuda.memory_reserved()/1e9\n",
        "        print(f\"CUDA Memory - allocated: {allocated:.2f} GB, reserved: {reserved:.2f} GB\")\n",
        "    else:\n",
        "        print(\"CUDA not available\")\n"
      ],
      "metadata": {
        "id": "a0jInVu5IqAp"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model_cpu = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
        "pipe_cpu = pipeline('text-generation', model=model_cpu, tokenizer=tokenizer, device=-1)\n",
        "\n",
        "print('Running CPU FP32...')\n",
        "text, t = run_generation(pipe_cpu, PROMPT, MAX_NEW_TOKENS)\n",
        "new_tokens = count_new_tokens(tokenizer, PROMPT, text)\n",
        "print(f\"Latency: {t:.3f}s | Approx new tokens: {new_tokens} | ~tokens/sec: {new_tokens/max(1e-6,t):.2f}\")\n",
        "print('\\nOUTPUT (truncated):\\n', text[:400])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWyyOw-xV5v9",
        "outputId": "cdd42bc9-d92c-4719-9cf3-0dc2d01c293a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running CPU FP32...\n",
            "Latency: 11.896s | Approx new tokens: 140 | ~tokens/sec: 11.77\n",
            "\n",
            "OUTPUT (truncated):\n",
            " In one sentence, explain why code reviews improve software quality.\n",
            "\n",
            "\"The best way to improve code quality is to make it easier to understand and understand the code,\" says Dr. David S. Karp, a professor of computer science at the University of California, Berkeley. \"If you can understand the code, you can understand the problem.\"\n",
            "\n",
            "The problem is that code reviews are often not as thorough as they\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    model_fp16 = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, device_map='auto')\n",
        "    pipe_fp16 = pipeline('text-generation', model=model_fp16, tokenizer=tokenizer)\n",
        "    show_gpu_mem()\n",
        "    print('Running GPU FP16...')\n",
        "    text, t = run_generation(pipe_fp16, PROMPT, MAX_NEW_TOKENS)\n",
        "    new_tokens = count_new_tokens(tokenizer, PROMPT, text)\n",
        "    print(f\"Latency: {t:.3f}s | Approx new tokens: {new_tokens} | ~tokens/sec: {new_tokens/max(1e-6,t):.2f}\")\n",
        "    show_gpu_mem()\n",
        "    print('\\nOUTPUT (truncated):\\n', text[:400])\n",
        "else:\n",
        "    print('No CUDA GPU detected; skip FP16 test.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fi6mnwRmV9Za",
        "outputId": "1f04ca78-c13b-437a-f689-2366bf691f94"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA Memory - allocated: 0.59 GB, reserved: 0.89 GB\n",
            "Running GPU FP16...\n",
            "Latency: 3.133s | Approx new tokens: 140 | ~tokens/sec: 44.69\n",
            "CUDA Memory - allocated: 0.59 GB, reserved: 0.89 GB\n",
            "\n",
            "OUTPUT (truncated):\n",
            " In one sentence, explain why code reviews improve software quality.\n",
            "\n",
            "\"The best way to improve code quality is to make it easier to understand and understand the code,\" says Dr. David S. Karp, a professor of computer science at the University of California, Berkeley. \"The more you understand the code, the more you can understand the code.\"\n",
            "\n",
            "The problem is that code reviews are not always easy to un\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    bnb_8 = BitsAndBytesConfig(load_in_8bit=True)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    model_int8 = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        quantization_config=bnb_8,\n",
        "        device_map='auto'\n",
        "    )\n",
        "    pipe_int8 = pipeline('text-generation', model=model_int8, tokenizer=tokenizer)\n",
        "    show_gpu_mem()\n",
        "    print('Running GPU INT8...')\n",
        "    text, t = run_generation(pipe_int8, PROMPT, MAX_NEW_TOKENS)\n",
        "    new_tokens = count_new_tokens(tokenizer, PROMPT, text)\n",
        "    print(f\"Latency: {t:.3f}s | Approx new tokens: {new_tokens} | ~tokens/sec: {new_tokens/max(1e-6,t):.2f}\")\n",
        "    show_gpu_mem()\n",
        "    print('\\nOUTPUT (truncated):\\n', text[:400])\n",
        "else:\n",
        "    print('No CUDA GPU detected; skip INT8 test.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNojgPomV-BD",
        "outputId": "c5e4f7d8-94e7-4294-cc6d-ad2023622702"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA Memory - allocated: 0.59 GB, reserved: 0.79 GB\n",
            "Running GPU INT8...\n",
            "Latency: 4.521s | Approx new tokens: 140 | ~tokens/sec: 30.96\n",
            "CUDA Memory - allocated: 0.59 GB, reserved: 0.79 GB\n",
            "\n",
            "OUTPUT (truncated):\n",
            " In one sentence, explain why code reviews improve software quality.\n",
            "\n",
            "\"The most important thing is to make sure that you're not making mistakes,\" says Dr. Michael J. Karp, a professor of computer science at the University of California, Berkeley. \"If you're not making mistakes, you're not making mistakes. You're making mistakes because you're making mistakes.\"\n",
            "\n",
            "The problem is that code reviews are \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    bnb_4 = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_quant_type='nf4',\n",
        "        bnb_4bit_use_double_quant=True\n",
        "    )\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "        model_int4 = AutoModelForCausalLM.from_pretrained(\n",
        "            MODEL_NAME,\n",
        "            quantization_config=bnb_4,\n",
        "            device_map='auto'\n",
        "        )\n",
        "        pipe_int4 = pipeline('text-generation', model=model_int4, tokenizer=tokenizer)\n",
        "        show_gpu_mem()\n",
        "        print('Running GPU INT4...')\n",
        "        text, t = run_generation(pipe_int4, PROMPT, MAX_NEW_TOKENS)\n",
        "        new_tokens = count_new_tokens(tokenizer, PROMPT, text)\n",
        "        print(f\"Latency: {t:.3f}s | Approx new tokens: {new_tokens} | ~tokens/sec: {new_tokens/max(1e-6,t):.2f}\")\n",
        "        show_gpu_mem()\n",
        "        print('\\nOUTPUT (truncated):\\n', text[:400])\n",
        "    except Exception as e:\n",
        "        print('INT4 load failed:', e)\n",
        "else:\n",
        "    print('No CUDA GPU detected; skip INT4 test.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIfARUNJWArL",
        "outputId": "f3b71f7a-dc1e-4d55-c01d-ed724179773b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA Memory - allocated: 0.64 GB, reserved: 0.78 GB\n",
            "Running GPU INT4...\n",
            "Latency: 1.910s | Approx new tokens: 140 | ~tokens/sec: 73.31\n",
            "CUDA Memory - allocated: 0.64 GB, reserved: 0.78 GB\n",
            "\n",
            "OUTPUT (truncated):\n",
            " In one sentence, explain why code reviews improve software quality.\n",
            "\n",
            "\"The best way to improve software quality is to make sure that the code is written in a way that is easy to understand and maintain,\" says the paper. \"The best way to do that is to make sure that the code is written in a way that is easy to understand and maintain.\"\n",
            "\n",
            "The paper's authors, from the University of California, Berkele\n"
          ]
        }
      ]
    }
  ]
}